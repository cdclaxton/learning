# Estimation of the state of an object.
#
# An object can be in one of a given number of states, but the state must be
# inferred from noisy data.
#
# The state of the object evolves over time, but once it has crossed to the next
# state in the sequence, it cannot return.
#
# Observations are provided about the object, but those observations are not
# under any control, so they arrive when they are generated by an external
# source. The external system may not send data it collects to the inference
# engine, but the probability of missing observations can be assumed to follow
# a parametric distribution.
#
# Observations from the external system do not uniquely define the state that
# the object is in, i.e. there is some ambiguity.
#
# The problem is to estimate the state of the object from the noisy and missing
# observations.

import itertools
import math
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np
import random
import scipy.optimize as optimize
import matplotlib.pyplot as plt


def stage_changepoints(num_stages, tau_max):
    """Returns a list of random stage changepoints."""

    assert num_stages > 1
    assert num_stages < tau_max, f"Too many stages ({num_stages}) for time {tau_max}"

    num_changepoints = num_stages - 1

    # Sample without replacement
    changepoints = random.sample(range(tau_max), num_changepoints)

    # Return a sorted list of changepoints in time order
    changepoints = sorted(changepoints)

    assert len(changepoints) == num_changepoints
    assert len(set(changepoints)) == len(
        changepoints
    ), f"Changepoints {changepoints} aren't unique"
    assert max(changepoints) < tau_max

    return changepoints


def gen_events_for_stage(t_min, t_max, p_event):
    """Generate events for the stage with time t_min <= t < t_max."""

    assert t_min >= 0
    assert t_min <= t_max, f"Invalid times {t_min}, {t_max}"
    assert abs(sum(p_event) - 1.0) < 1e-6, f"Probability doesn't sum to 1"

    if t_min == t_max:
        return np.array([]), np.array([], dtype=int)

    # Number of events in the stage to generate (minimum of 1)
    prop = np.random.uniform(0, 0.8)
    tau = t_max - t_min
    num_events = max(math.floor(prop * tau), 1)

    # Times of the events
    event_times = sorted(random.sample(range(t_min, t_max), num_events))
    assert len(event_times) == num_events
    assert (
        len(set(event_times)) == num_events
    ), f"Event times aren't unique: {event_times}"
    assert min(event_times) >= t_min
    assert max(event_times) < t_max

    # Types of events
    event_types = [
        int(np.where(np.random.multinomial(1, p_event) == 1)[0][0])
        for _ in range(num_events)
    ]
    assert (
        len(event_types) == num_events
    ), f"Got {len(event_types)}, expected {num_events}"
    assert min(event_types) >= 0
    assert max(event_types) < len(p_event)

    # Return a tuple of event times and event types
    assert len(event_times) == len(event_types)
    return event_times, event_types


def check_cpt_is_valid(cpt):
    """Check that the CPT is valid, i.e. rows sum to 1."""

    # Difference between the sum of the rows and 1.0
    return max(np.sum(cpt, axis=1) - 1.0) < 1e-6


def check_probs_sum_to_1(p):
    """Check that a vector of probabilities sums to 1."""
    return abs(sum(p) - 1.0) < 1e-6


def generate_obs(p_s, cpt, tau_max):
    """Generate synthetic observations."""

    assert check_probs_sum_to_1(p_s), f"Probabilities don't sum to 1"
    assert tau_max > 0, f"Invalid tau_max: {tau_max}"
    assert check_cpt_is_valid(cpt), f"Row(s) don't sum to 1: {cpt}"
    assert len(p_s) == cpt.shape[0]

    # Sample to get the number of stages
    num_stages = np.where(np.random.multinomial(1, p_s) == 1)[0][0] + 1
    assert num_stages >= 1
    assert num_stages <= len(p_s)

    if num_stages == 1:
        event_times, event_types = gen_events_for_stage(0, tau_max, cpt[0, :])
        gt_stages = np.repeat(0, len(event_times))
        changepoints = []
    else:
        # Determine the stage changepoints
        changepoints = stage_changepoints(num_stages, tau_max)

        # Generate the ground truth stage, event time, event type
        gt_stages = np.array([])
        event_times = np.array([])
        event_types = np.array([], dtype=int)

        # Walk through each stage
        for s in range(num_stages):
            if s == 0:
                # First stage
                t_min = 0
                t_max = changepoints[0]
            elif s == num_stages - 1:
                # Last stage
                t_min = changepoints[-1]
                t_max = tau_max
            else:
                t_min = changepoints[s - 1]
                t_max = changepoints[s]

            assert s < cpt.shape[0], f"Insufficient rows in CPT for stage {s}"
            event_times_s, event_types_s = gen_events_for_stage(t_min, t_max, cpt[s, :])

            num_events = len(event_times_s)

            gt_stages = np.append(gt_stages, np.repeat(s, num_events))
            event_times = np.append(event_times, event_times_s)
            event_types = np.append(event_types, event_types_s)

        # Ensure there are no missing stages
        assert (
            len(set(gt_stages)) == num_stages
        ), f"Expected {num_stages}, got {set(gt_stages)}"

    assert len(gt_stages) == len(
        event_times
    ), f"gt_stages = {gt_stages}, event_times = {event_times}"
    assert len(gt_stages) == len(
        event_types
    ), f"gt_stages = {gt_stages}, event_types = {event_types}"

    return gt_stages, event_times, event_types, changepoints


def indicator(event_time, stage, changepoints, tau_max):
    """Indicator function returns 1 if event time is between changepoints, otherwise 0."""

    assert (
        0 <= event_time <= tau_max
    ), f"event time {event_time} out of range [0, {tau_max}]"
    assert tau_max > 0

    # If there are no changepoints, then the event is definitely within the stage
    if len(changepoints) == 0:
        return 1.0

    if stage == 0:
        t_min = 0
        t_max = changepoints[0]
    elif stage == len(changepoints):
        t_min = changepoints[-1]
        t_max = tau_max + 1
    else:
        t_min = changepoints[stage - 1]
        t_max = changepoints[stage]

    if t_min <= event_time < t_max:
        return 1.0
    else:
        return 0.0


def log_likelihood(event_times, event_types, changepoints, cpt, tau_max):
    """Calculate the log-likelihood."""

    assert len(event_times) == len(event_types)
    assert tau_max > 0

    total = 0.0
    for s in range(len(changepoints) + 1):
        for k in range(len(event_times)):
            assert s < cpt.shape[0], f"Stage {s} out of range for CPT"
            assert (
                event_types[k] < cpt.shape[1]
            ), f"Event type {event_types[k]} out of range for CPT"

            # Add on a small number in case the probability from the CPT is 0
            p = cpt[s, event_types[k]] + 1e-16
            total += indicator(event_times[k], s, changepoints, tau_max) * math.log(p)

    return total


def valid_changepoints(seq):
    """Does the sequence represent valid changepoints?"""

    if len(seq) == 1:
        return True

    for i in range(len(seq) - 1):
        if seq[i] >= seq[i + 1]:
            return False

    return True


def all_changepoints(num_changepoints, tau_max):
    """Construct a list of all possible valid changepoint positions."""

    assert num_changepoints >= 0
    assert tau_max >= 0

    if num_changepoints == 0:
        return []

    if tau_max == 0:
        return []

    changepoints = []

    x = list(range(tau_max))
    y = [x for _ in range(num_changepoints)]
    for seq in itertools.product(*y):
        if valid_changepoints(seq):
            changepoints.append(seq)

    return changepoints


def run_inference(event_times, event_types, p_s, cpt, tau_max):
    """Run inference."""

    assert len(event_times) == len(event_types)
    assert len(p_s) > 0
    assert len(p_s) == cpt.shape[0], f"incompatible number of stages"
    assert tau_max >= 0

    joint_probs = []

    if len(event_times) == 0 or tau_max == 0:
        for s in range(len(p_s)):
            joint_probs.append((s + 1, [], p_s[s]))
        return joint_probs

    # Maximum number of changepoints
    num_changepoints = len(p_s) - 1

    changepoints_to_process = all_changepoints(num_changepoints, tau_max)

    if len(changepoints_to_process) == 0:
        for s in range(len(p_s)):
            joint_probs.append((s + 1, [], p_s[s]))
        return joint_probs

    # Walk through each possible configuration of changepoints
    for changepoints in changepoints_to_process:
        # Walk through the number of stages in the model
        for s in range(len(p_s)):
            select_changepoints = changepoints[:s]

            ll = log_likelihood(
                event_times, event_types, select_changepoints, cpt, tau_max
            )
            prior_s = p_s[s]
            prior_tau = 1.0
            joint = math.exp(ll) * prior_s * prior_tau
            joint_probs.append((s + 1, changepoints, joint))

    assert len(joint_probs) > 0, f"joint_probs = {joint_probs}, tau_max = {tau_max}"

    return joint_probs


def p_num_stages(joint_probs, L):
    """Probability of the number of stages from the joint."""

    assert L > 0, f"max number of stages invalid: {L}"

    total_for_num_stages = np.repeat(0.0, L)
    total = 0.0

    for j in joint_probs:
        num_stages, _, joint = j
        total_for_num_stages[num_stages - 1] += joint
        total += joint

    assert total > 0.0, f"total = 0 for joint_probs: {joint_probs}"
    return total_for_num_stages / total


def trim_events(event_times, event_types, t):
    """Retain events up to and including a given time."""

    assert len(event_times) == len(event_types)
    assert t >= 0

    event_times_in_range = np.array([])
    event_types_in_range = np.array([], dtype=int)

    for i in range(len(event_times)):
        if event_times[i] <= t:
            event_times_in_range = np.append(event_times_in_range, event_times[i])
            event_types_in_range = np.append(event_types_in_range, event_types[i])

    return event_times_in_range, event_types_in_range


def prob_num_stages_over_time(event_times, event_types, p_s, cpt, tau_max):
    """Calculate the probability of the number of stages over time."""

    # Matrix to hold the probability of the number of stages
    m = np.zeros((len(p_s), tau_max))

    for t in range(tau_max):
        # Only retain the events up to and including time t
        event_times_in_range, event_types_in_range = trim_events(
            event_times, event_types, t
        )

        # Run inference
        joint_probs = run_inference(
            event_times_in_range, event_types_in_range, p_s, cpt, t
        )
        assert len(joint_probs) > 0

        # Summarise
        m[:, t] = p_num_stages(joint_probs, len(p_s))

    return m


def calc_cpt_event_given_stage(cpt_s_given_e, p_s):
    """Calculate the matrix of p(e|s) from p(s|e) and p(s)."""

    assert check_cpt_is_valid(cpt_s_given_e)
    assert check_probs_sum_to_1(p_s)

    cpt_s_given_e_t = cpt_s_given_e.transpose()

    # Number of different types of events
    N = cpt_s_given_e.shape[0]

    # Number of different stages
    M = cpt_s_given_e.shape[1]

    # Function to calculate the error given a candidate p(e)
    # Error term is really np.dot(y,y), but better results were found by scaling
    def f(e):
        y = np.dot(cpt_s_given_e_t, e) - p_s
        return 100 * np.dot(y, y) ** 2

    # Constraints
    cons = {"type": "eq", "fun": lambda x: x.sum() - 1}  # p(e) must sum to 1

    # Bounds (each 0 <= p(e_i) <= 1)
    bnds = [(0, 1) for _ in range(N)]

    # Perform optimisation to find p(e)
    res = optimize.minimize(
        f,
        np.ones(N) / N,  # initial
        method="SLSQP",
        constraints=cons,
        bounds=bnds,
        tol=1e-30,
        options={"disp": False, "maxiter": 10000, "ftol": 1e-30, "eps": 1e-10},
    )
    print(res)
    p_e_best = res["x"]

    # Calculate p(s) given p(s|e) and the estimated p(e)
    p_s_est = np.zeros(M)

    # Calculate the CPT p(e|s)
    cpt_e_given_s = np.zeros((M, N))
    for s in range(M):
        for e in range(N):
            cpt_e_given_s[s, e] = cpt_s_given_e[e, s] * p_e_best[e] / p_s[s]

    print(cpt_e_given_s)

    # Check each row of the CPT sums to 1
    assert check_cpt_is_valid(
        cpt_e_given_s
    ), f"CPT isn't valid: {cpt_e_given_s}, row sum: {np.sum(cpt_e_given_s, axis=1)}"

    return cpt_e_given_s, p_e_best


def demo_of_optimisation_problem():
    """Plot the surface of the error for different values of p(e)."""

    # CPT of p(s|e) with 3 events and 2 stages
    cpt_s_given_e = np.array([[0.1, 0.9], [0.8, 0.2], [0.6, 0.4]])

    p_s = np.array([0.64, 0.36])

    cpt_s_given_e_t = cpt_s_given_e.transpose()

    def f(e):
        y = np.dot(cpt_s_given_e_t, e) - p_s
        return np.dot(y, y)

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

    # Make data
    X = np.arange(0, 1, 0.1)
    Y = np.arange(0, 1, 0.1)
    X, Y = np.meshgrid(X, Y)

    Z = np.zeros(X.shape)
    for x in range(X.shape[1]):
        for y in range(X.shape[0]):
            Z[x, y] = f(np.array([x, y, 1 - (x + y)]))

    # Plot the surface
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

    # Add a color bar
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()


if __name__ == "__main__":
    # Show the surface on which optimisation is performed
    # demo_of_optimisation_problem()

    # Number of time steps
    tau_max = 40

    # Test case to demonstrate
    test_case = 1

    if test_case == 0:
        # Distribution of the number of stages
        #               1    2    3
        p_s = np.array([0.6, 0.3, 0.1])

        # Distribution of the number of stages when generating synthetic data
        #                   1    2    3
        p_s_gen = np.array([0.2, 0.2, 0.6])

        # CPT defining the probability of a given event type given the stage
        # Each row is an event and each column is a stage and so the rows must sum
        # to 1
        #    1    2    3  <-- stage
        cpt_stage_given_event = np.array(
            [
                [0.2, 0.1, 0.7],
                [0.3, 0.7, 0.0],
                [0.7, 0.1, 0.2],
                [0.1, 0.4, 0.5],
                [0.3, 0.3, 0.4],
            ]
        )

        # Use optimisation to find p(e|s)
        cpt_e_given_s, p_e = calc_cpt_event_given_stage(cpt_stage_given_event, p_s)

    elif test_case == 1:
        # Example case: Two stages, 4 events
        cpt_e_given_s = np.array([[0.2, 0.3, 0.4, 0.1], [0.1, 0.7, 0.1, 0.1]])
        p_s = np.array([0.8, 0.2])
        p_s_gen = np.array([0.2, 0.8])

    # Generate the ground truth stage and observed events
    gt_stages, event_times, event_types, gt_changepoints = generate_obs(
        p_s_gen, cpt_e_given_s, tau_max
    )
    print(f"Ground truth stages: {gt_stages}")
    print(f"Event times: {event_times}")
    print(f"Event types: {event_types}")
    print(f"Changepoints: {gt_changepoints}")

    # Calculate the probability of the number of stages over time
    m = prob_num_stages_over_time(event_times, event_types, p_s, cpt_e_given_s, tau_max)

    # Plot the ground truth vs. the inferred state
    fig = plt.figure()

    plt.subplot(1, 2, 1)
    plt.plot(event_times, event_types, "x")
    plt.xlim(0, tau_max)
    for c in gt_changepoints:
        plt.axvline(x=c, color="r", ls=":")
    plt.xlabel("Time index")
    plt.ylabel("Event type")

    plt.subplot(1, 2, 2)
    for num_cps in range(m.shape[0]):
        plt.plot(m[num_cps, :], label=f"{num_cps + 1} stages")
    for c in gt_changepoints:
        plt.axvline(x=c, color="r", ls=":")
    for e in event_times:
        plt.axvline(x=e, color="k", ls="--", alpha=0.2)
    plt.xlim(0, tau_max)
    plt.xlabel("Time index")
    plt.ylabel("Probability of stage")
    plt.legend()

    plt.show()
